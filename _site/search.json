[
  {
    "objectID": "SETUP-COMMENTS.html",
    "href": "SETUP-COMMENTS.html",
    "title": "How to Set Up Comments with Notifications",
    "section": "",
    "text": "Your blog now has a comment system using giscus, which is powered by GitHub Discussions. This means you‚Äôll receive notifications when someone comments!\n\n\n\n\nIf you haven‚Äôt already: - Go to GitHub and create a new public repository - Push your blog code to this repository\n\n\n\n\nGo to your repository on GitHub\nClick Settings\nScroll down to Features\nCheck the box for Discussions\n\n\n\n\n\nVisit https://giscus.app/\nEnter your repository name (e.g., username/blog-repo)\nChoose Discussion Category: Select ‚ÄúGeneral‚Äù or create a new category\nThe page will generate configuration values for you\n\n\n\n\nOpen comments.html and replace these placeholders with your values: - YOUR-GITHUB-USERNAME/YOUR-REPO-NAME - Your GitHub username and repo name - YOUR-REPO-ID - From giscus configuration page - YOUR-CATEGORY-ID - From giscus configuration page\nExample:\ndata-repo=\"yourusername/yourblog\"\ndata-repo-id=\"R_kgDOxxxxxx\"\ndata-category=\"General\"\ndata-category-id=\"DIC_kwDOxxxxxx\"\n\n\n\nOnce set up, you‚Äôll automatically receive: - Email notifications when someone comments (via GitHub) - GitHub notifications in your GitHub notifications panel - Can customize notification settings in GitHub Settings &gt; Notifications\n\n\n\nTo add comments and share buttons to existing posts, add this to their frontmatter:\nformat:\n  html:\n    include-after-body:\n      - file: ../share-buttons.html\n      - file: ../comments.html\n\n\n\n\nIf you prefer not to use GitHub:\n\nUtterances - Similar to giscus but uses GitHub Issues\n\nVisit: https://utteranc.es/\n\nDisqus - Traditional comment system\n\nVisit: https://disqus.com/\nHas ads in free tier\n\nCommento - Privacy-focused\n\nVisit: https://commento.io/\nSelf-hosted or paid hosting\n\n\n\n\n\n‚úÖ Email notifications for new comments ‚úÖ Markdown support in comments ‚úÖ Reactions (üëç, ‚ù§Ô∏è, etc.) ‚úÖ Dark mode support (syncs with your site theme) ‚úÖ No tracking or ads ‚úÖ Free to use ‚úÖ Moderation tools via GitHub"
  },
  {
    "objectID": "SETUP-COMMENTS.html#setup-steps",
    "href": "SETUP-COMMENTS.html#setup-steps",
    "title": "How to Set Up Comments with Notifications",
    "section": "",
    "text": "If you haven‚Äôt already: - Go to GitHub and create a new public repository - Push your blog code to this repository\n\n\n\n\nGo to your repository on GitHub\nClick Settings\nScroll down to Features\nCheck the box for Discussions\n\n\n\n\n\nVisit https://giscus.app/\nEnter your repository name (e.g., username/blog-repo)\nChoose Discussion Category: Select ‚ÄúGeneral‚Äù or create a new category\nThe page will generate configuration values for you\n\n\n\n\nOpen comments.html and replace these placeholders with your values: - YOUR-GITHUB-USERNAME/YOUR-REPO-NAME - Your GitHub username and repo name - YOUR-REPO-ID - From giscus configuration page - YOUR-CATEGORY-ID - From giscus configuration page\nExample:\ndata-repo=\"yourusername/yourblog\"\ndata-repo-id=\"R_kgDOxxxxxx\"\ndata-category=\"General\"\ndata-category-id=\"DIC_kwDOxxxxxx\"\n\n\n\nOnce set up, you‚Äôll automatically receive: - Email notifications when someone comments (via GitHub) - GitHub notifications in your GitHub notifications panel - Can customize notification settings in GitHub Settings &gt; Notifications\n\n\n\nTo add comments and share buttons to existing posts, add this to their frontmatter:\nformat:\n  html:\n    include-after-body:\n      - file: ../share-buttons.html\n      - file: ../comments.html"
  },
  {
    "objectID": "SETUP-COMMENTS.html#alternative-comment-systems",
    "href": "SETUP-COMMENTS.html#alternative-comment-systems",
    "title": "How to Set Up Comments with Notifications",
    "section": "",
    "text": "If you prefer not to use GitHub:\n\nUtterances - Similar to giscus but uses GitHub Issues\n\nVisit: https://utteranc.es/\n\nDisqus - Traditional comment system\n\nVisit: https://disqus.com/\nHas ads in free tier\n\nCommento - Privacy-focused\n\nVisit: https://commento.io/\nSelf-hosted or paid hosting"
  },
  {
    "objectID": "SETUP-COMMENTS.html#features-you-get",
    "href": "SETUP-COMMENTS.html#features-you-get",
    "title": "How to Set Up Comments with Notifications",
    "section": "",
    "text": "‚úÖ Email notifications for new comments ‚úÖ Markdown support in comments ‚úÖ Reactions (üëç, ‚ù§Ô∏è, etc.) ‚úÖ Dark mode support (syncs with your site theme) ‚úÖ No tracking or ads ‚úÖ Free to use ‚úÖ Moderation tools via GitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bhabishya Neupane",
    "section": "",
    "text": "Data engineer exploring statistical inference, interactive visualizations, and LLM optimization. My posts cover practical applications from bootstrap resampling to deploying LLMs on GPUs.\n\n\n\n\nBootstrap inference, exploratory data analysis, and hypothesis testing with R\n\n\n\nvLLM optimization, KV cache management, and serving models on A100 GPUs\n\n\n\nBuilding Shiny applications for data exploration and time series forecasting\n\n\n\nRead My Posts Read About Me"
  },
  {
    "objectID": "index.html#what-i-write-about",
    "href": "index.html#what-i-write-about",
    "title": "Bhabishya Neupane",
    "section": "",
    "text": "Data engineer exploring statistical inference, interactive visualizations, and LLM optimization. My posts cover practical applications from bootstrap resampling to deploying LLMs on GPUs.\n\n\n\n\nBootstrap inference, exploratory data analysis, and hypothesis testing with R\n\n\n\nvLLM optimization, KV cache management, and serving models on A100 GPUs\n\n\n\nBuilding Shiny applications for data exploration and time series forecasting\n\n\n\nRead My Posts Read About Me"
  },
  {
    "objectID": "posts/2022-03-28-tidy-tuesday-world-freedom-index/tidy-tuesday-world-freedom-index.html",
    "href": "posts/2022-03-28-tidy-tuesday-world-freedom-index/tidy-tuesday-world-freedom-index.html",
    "title": "Tidy Tuesday: World Freedom Index",
    "section": "",
    "text": "# Analysis Pack\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(scales)\nlibrary(infer)\n# Theming Pack\nlibrary(nationalparkcolors)\nlibrary(paletteer)\n# Styling Pack\nlibrary(reactable)\nlibrary(reactablefmtr)\nlibrary(showtext)\nlibrary(showtextdb)\nlibrary(extrafont)\nlibrary(extrafontdb)\nlibrary(gganimate)\nfont_add_google(\"BenchNine\", family = \"BenchNine\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/2022-03-28-tidy-tuesday-world-freedom-index/tidy-tuesday-world-freedom-index.html#maps",
    "href": "posts/2022-03-28-tidy-tuesday-world-freedom-index/tidy-tuesday-world-freedom-index.html#maps",
    "title": "Tidy Tuesday: World Freedom Index",
    "section": "Maps",
    "text": "Maps\n\nWorld Freedom Index: Political Rights Score (2020)\ncountrycode package along with map_data(\"world\") data from ggplot2 allows for easy\nmapping methods.\n\nlibrary(countrycode)\nfreedom_joined &lt;- freedom %&gt;% \n  mutate(country_code = countrycode(country, \"country.name\", \"iso2c\")) %&gt;% \n  left_join(map_data(\"world\"), by = c(\"country\" = \"region\"), suffix = c(\"\", \"_map\"))\n\nWarning in left_join(., map_data(\"world\"), by = c(country = \"region\"), suffix = c(\"\", : Each row in `x` is expected to match at most 1 row in `y`.\n‚Ñπ Row 1 of `x` matches multiple rows.\n‚Ñπ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\nfreedom_joined %&gt;% \n  filter(year == 2020) %&gt;% \n  ggplot(aes(long, lat, group = group))+\n  geom_polygon(aes(fill = political_rights))+\n  ggthemes::theme_map()+\n  coord_map(xlim = c(-180, 180))+\n  scale_fill_gradient2(low = \"#A8CDEC\", high = \"#F6955E\", midpoint = 3.5, \n                       guide = guide_legend(reverse = TRUE))+\n  labs(fill = \"Political Rights\", \n       title = \"World Freedom Index: Political Rights Score (2020)\", \n       subtitle = \"Lower scores imply country is more free and vice-versa\")+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\nWorld Freedom Index: Civil Liberties Score Score (2020)\n\nfreedom_joined %&gt;% \n  filter(year == 2020) %&gt;% \n  ggplot(aes(long, lat, group = group))+\n  geom_polygon(aes(fill = civil_liberties))+\n  ggthemes::theme_map()+\n  coord_map(xlim = c(-180, 180))+\n  scale_fill_gradient2(low = \"#A8CDEC\", high = \"#F6955E\", midpoint = 3.5, \n                       guide = guide_legend(reverse = TRUE))+\n  labs(fill = \"Civil Liberties\", \n       title = \"World Freedom Index: Civil Liberties Score (2020)\", \n       subtitle = \"Lower scores imply country is more free and vice-versa\")+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\nWorld Freedom Index: Developed vs Least Developed Countries (2020)\n\nfreedom_joined %&gt;% \n  filter(year == 2020) %&gt;% \n  ggplot(aes(long, lat, group = group))+\n  geom_polygon(aes(fill = is_ldc))+\n  ggthemes::theme_map()+\n  coord_map(xlim = c(-180, 180))+\n  scale_fill_paletteer_d(\"nationalparkcolors::Arches\")+\n  labs(fill = \"\", \n       title = \"World Freedom Index: Developed vs Least Developed Countries (2020)\", \n       subtitle = \"Africa trumps all the regions when it comes to # of least developed countries\")+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/2022-03-28-tidy-tuesday-art-collections/tidy-tuesday-art-collections.html",
    "href": "posts/2022-03-28-tidy-tuesday-art-collections/tidy-tuesday-art-collections.html",
    "title": "Tidy Tuesday: Art Collection",
    "section": "",
    "text": "Libraries\nLoading in my go-to libraries.\n\n# Analysis Pack\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(scales)\nlibrary(infer)\n# Theming Pack\nlibrary(nationalparkcolors)\nlibrary(paletteer)\n# Styling Pack\nlibrary(reactable)\nlibrary(reactablefmtr)\nlibrary(showtext)\nlibrary(showtextdb)\nlibrary(extrafont)\nlibrary(extrafontdb)\nfont_add_google(\"BenchNine\", family = \"BenchNine\")\nshowtext_auto()\n\n\n\nData\n\nartwork &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\nartists &lt;- readr::read_csv(\"https://github.com/tategallery/collection/raw/master/artist_data.csv\")\n\n\nartwork_artist_tbl &lt;- artwork %&gt;% \n  left_join(artists, by = c(\"artist\" = \"name\"), suffix = c(\"\", \"_ar_df\")) %&gt;% \n  separate(placeOfBirth, into = c(\"city\", \"country\"), sep = \", \", extra = \"merge\")\nartwork_artist_tbl %&gt;% \n  filter(!is.na(country)) %&gt;% \n  count(country, sort = T) %&gt;% \n  reactable::reactable(\n    theme = espn(font_size = 12, centered = TRUE), \n    \n    columns = list(\n      \n      country = colDef(name = \"Country\", align = \"center\"),\n      n       = colDef(name = \"No. of Occurrences\", align = \"center\")\n      \n    )\n  )\n\n\n\n\n\nTate Art Museum is a museum based in the United Kingdom, so I am not really surprised to see the UK arts being reported more frequently in this dataset.\n\nartwork_artist_tbl %&gt;% \n  count(country, artist, sort = T) |&gt; \n  reactable(\n    theme = espn(font_size = 12), \n    \n    columns = list(\n      country = colDef(name = \"Country\", align = \"center\"), \n      \n      artist = colDef(name = \"Artist\", align = \"center\"), \n      \n      n      = colDef(name = \"No. of Occurrences\", align = \"center\")\n    )\n  )\n\n\n\n\n\nA lot of the United Kingdom‚Äôs records in this dataset comes from Turner, Joseph Mallord William. According to Wikipedia, he was known as romantic painter, print maker, and water colorist. I am googling his paintings right now, and they are something.\n\n\nStatistical Inference\nI am interested in learning more about the average dimension of the arts (average height, width, and depth). I think one way to go about this would be to collect all the arts available in the world and measure their dimension which is not just super expensive but also impossible. Another way to learn about the average dimension of the art would be to create bootstrap resamples and calculate their means.\n\nAssumptions:\n\nI am assuming this dataset to be randomly sampled, so that any inference made using this sample can be generalized.\nI am assuming this dataset to be representative of all of the art works around the world.\n\nMy population parameter of interest is the average dimension of an art which is unknown. However, I have this dataset as a sample. I am going to consider using this sample to create bootstrapped resamples and calculate the average dimension on those bootstrapped resamples. In order to proceed ahead, I am going to use infer for virtual bootstrap simulation and inference. Go check out infer package!!!\n\n\n\nDistribution of dimensions from the sample\n\nartwork_artist_tbl %&gt;%\n  filter(!is.na(height)) %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(color = \"white\", fill = \"#F6955E\", alpha = 0.8) +\n  scale_x_log10() +\n  geom_vline(xintercept = artwork_artist_tbl %&gt;% summarize(avg_height = mean(height, na.rm = T)) %&gt;% pull(avg_height), lty = 2, size = 1.5)+\n  theme_minimal()+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))+\n  labs(x = \"Height (mm)\", \n       y = \"# of arts\", \n       title = \"Distribution of Height\", \n       subtitle = \"The dashed line represents the average height of art works from original sample i.e. 346.88 mm, and x-axis is log scaled\")\n\n\n\n\n\n\n\n\n\nartwork_artist_tbl %&gt;%\n  filter(!is.na(width)) %&gt;%\n  ggplot(aes(width)) +\n  geom_histogram(color = \"white\", fill = \"#F6955E\", alpha = 0.8) +\n  scale_x_log10() +\n  geom_vline(xintercept = artwork_artist_tbl %&gt;% summarize(avg_width = mean(width, na.rm = T)) %&gt;% pull(avg_width), lty = 2, size = 1.5)+\n  theme_minimal()+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))+\n  labs(x = \"Width (mm)\", \n       y = \"# of arts\", \n       title = \"Distribution of Width\", \n       subtitle = \"The dashed line represents the average depth of artworks from original sample i.e. 323.81 mm, and x-axis is log scaled\")\n\n\n\n\n\n\n\n\n\nartwork_artist_tbl %&gt;%\n  filter(!is.na(depth)) %&gt;%\n  ggplot(aes(depth)) +\n  geom_histogram(color = \"white\", fill = \"#F6955E\", alpha = 0.8) +\n  scale_x_log10() +\n  geom_vline(xintercept = artwork_artist_tbl %&gt;% summarize(avg_depth = mean(depth, na.rm = T)) %&gt;% pull(avg_depth), lty = 2, size = 1.5)+\n  theme_minimal()+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5), \n        plot.subtitle = element_text(hjust = 0.5))+\n  labs(x = \"Depth (mm)\", \n       y = \"# of arts\", \n       title = \"Distribution of Depth\", \n       subtitle = \"The dashed line represents the average depth of art works from original sample i.e. 477.11 mm, and x-axis is log scaled\")\n\n\n\n\n\n\n\n\nSince, I have assumed this dataset to be representative of all art works around the world. I think a good starting point to think of average dimension would be to calculate the averages from this dataset which is 346.88 mm in height, 323.81 mm in width, and 477.11 mm in depth.\n\n\nResampling\nI will perform a 1000 bootstrap resample on my original sample which is this dataset and calculate averages on the resamples for height, width, and depth.\nI am not going to get into explaining what confidence interval and p-values are. I think a good place to learn about them using this similar workflow would be Statistical Inference via Data Science. I would highly recommend checking out this book if you want to have a clear concept on standard error, confidence intervals, resampling, bootstraps, hypothesis testing, etc.\n\nBootstrap Distribution of Height\n\nartwork_height_bootstrapped_means &lt;- artwork_artist_tbl %&gt;% \n  filter(!is.na(height)) %&gt;%\n  rowid_to_column(var = \"ID\") %&gt;% \n  select(ID, height) %&gt;% \n  specify(response = height) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"mean\")\n\n\nheight_ci_endpoints &lt;- artwork_height_bootstrapped_means %&gt;% \n  get_ci()\nartwork_height_bootstrapped_means %&gt;%\n  ggplot(aes(stat)) +\n  geom_histogram(fill = \"#F6955E\", color = \"white\") +\n  shade_ci(\n    endpoints = height_ci_endpoints,\n    color = \"midnightblue\",\n    fill = \"#A8CDEC\",\n    linetype = \"dashed\"\n  ) +\n  labs(title   = \"Simulation-Based Bootstrap Distribution of Height\",\n       y       = \"# of resamples\",\n       x       = \"Resampled Means\") +\n  geom_vline(\n    xintercept = artwork %&gt;% summarize(avg_height = mean(height, na.rm = T)) %&gt;% pull(avg_height),\n    color = \"midnightblue\",\n    size  = 1.5,\n    lty = 5\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"BenchNine\"),\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nBootstrap Distribution of Width\n\nartwork_width_bootstrapped_means &lt;- artwork_artist_tbl %&gt;% \n  filter(!is.na(width)) %&gt;%\n  rowid_to_column(var = \"ID\") %&gt;% \n  select(ID, width) %&gt;% \n  specify(response = width) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% # generating 1000 bootstrapped \n  calculate(stat = \"mean\")\n\n\nwidth_ci_endpoints &lt;- artwork_width_bootstrapped_means %&gt;% \n  get_ci()\nartwork_width_bootstrapped_means %&gt;%\n  ggplot(aes(stat)) +\n  geom_histogram(fill = \"#F6955E\", color = \"white\") +\n  shade_ci(\n    endpoints = width_ci_endpoints,\n    color = \"midnightblue\",\n    fill = \"#A8CDEC\",\n    linetype = \"dashed\"\n  ) +\n  labs(title   = \"Simulation-Based Bootstrap Distribution of Width\",\n       y       = \"# of resamples\",\n       x       = \"Resampled Means\") +\n  geom_vline(\n    xintercept = artwork %&gt;% summarize(avg_width = mean(width, na.rm = T)) %&gt;% pull(avg_width),\n    color = \"midnightblue\",\n    size  = 1.5,\n    lty = 5\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"BenchNine\"),\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nBootstrap Distribution of Depth\n\nartwork_depth_bootstrapped_means &lt;- artwork_artist_tbl %&gt;% \n  filter(!is.na(depth)) %&gt;%\n  rowid_to_column(var = \"ID\") %&gt;% \n  select(ID, depth) %&gt;% \n  specify(response = depth) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% # generating 1000 bootstrapped \n  calculate(stat = \"mean\")\n\n\ndepth_ci_endpoints &lt;- artwork_depth_bootstrapped_means %&gt;% \n  get_ci()\nartwork_depth_bootstrapped_means %&gt;%\n  ggplot(aes(stat)) +\n  geom_histogram(fill = \"#F6955E\", color = \"white\") +\n  shade_ci(\n    endpoints = depth_ci_endpoints,\n    color = \"midnightblue\",\n    fill = \"#A8CDEC\",\n    linetype = \"dashed\"\n  ) +\n  labs(title   = \"Simulation-Based Bootstrap Distribution of Depth\",\n       y       = \"# of resamples\",\n       x       = \"Resampled Means\") +\n  geom_vline(\n    xintercept = artwork %&gt;% summarize(avg_depth = mean(depth, na.rm = T)) %&gt;% pull(avg_depth),\n    color = \"midnightblue\",\n    size  = 1.5,\n    lty = 5\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"BenchNine\"),\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nFrom the simulated plots above, we can see that if we were to repeat the same sampling procedure a large number of times, we expect about 95% of the intervals to capture the value of the true population parameter. In this case, we can see our original sample mean is captured in the 95% confidence interval (percentile method)."
  },
  {
    "objectID": "posts/2022-03-30-r-shiny-small-business-innovation-research/r-shiny-small-business-innovation-research.html",
    "href": "posts/2022-03-30-r-shiny-small-business-innovation-research/r-shiny-small-business-innovation-research.html",
    "title": "R Shiny: Small Business Innovation Research",
    "section": "",
    "text": "Dashboard Link\nLink to the Shiny Application: Small Business Innovation Research Dashboard\n\n\n\n\n\n\n\n\n\n\n\nAbout the Project\nThis shiny dashboard was put together in a very short notice. I worked on this project two weeks before my graduation and there were a lot of changes that were made on this dashboard.\n\nAbout SBIR\nThe Small Business Innovation Research (SBIR) programs is a competitive program that encourages small businesses to engage in Federal Research/Research and Development (R/R&D) with the potential for commercialization. Through a competitive awards-based program, SBIR awards enable small businesses to explore their technological potential and provide the incentive to profit from its commercialization.\n\n\nResources that helped me\n\nInteractive chloropleth map with leaflet and shiny"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nJan 3, 2026\n\n\nvLLM and LMCache: Notes on Optimizing LLM Inference\n\n\nBhabishya Neupane\n\n\n7 min\n\n\n\n\n\n\nMar 30, 2022\n\n\nMapping Federal SBIR Funding Across States and Departments\n\n\nBhabishya Neupane\n\n\n1 min\n\n\n\n\n\n\nMar 28, 2022\n\n\nInteractive Dashboard: Visualizing World Freedom Index Over Time\n\n\nBhabishya Neupane\n\n\n1 min\n\n\n\n\n\n\nMar 28, 2022\n\n\nBuilding an R Data Package for Dairy Processing Research\n\n\nBhabishya Neupane\n\n\n1 min\n\n\n\n\n\n\nMar 28, 2022\n\n\nTime Series Forecasting: US Natural Gas Demand with ARIMA and TBATS\n\n\nBhabishya Neupane\n\n\n1 min\n\n\n\n\n\n\nMar 28, 2022\n\n\nAnalyzing Civil Liberty and Political Rights Across Regions\n\n\nBhabishya Neupane\n\n\n11 min\n\n\n\n\n\n\nMar 27, 2022\n\n\nExploring Chocolate Ratings from The Manhattan Chocolate Society\n\n\nBhabishya Neupane\n\n\n8 min\n\n\n\n\n\n\nMar 26, 2022\n\n\nBootstrap Inference on Tate Gallery Art Dimensions\n\n\nBhabishya Neupane\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-03-28-shiny-application-world-freedom-index/shiny-application-world-freedom-index.html",
    "href": "posts/2022-03-28-shiny-application-world-freedom-index/shiny-application-world-freedom-index.html",
    "title": "R Shiny Application: World Freedom Index",
    "section": "",
    "text": "Link\nLink: World Freedom Index Shiny Dashboard"
  },
  {
    "objectID": "posts/2022-03-28-tidy-tuesday-chocolate-ratings/tidy-tuesday-chocolate-ratings.html",
    "href": "posts/2022-03-28-tidy-tuesday-chocolate-ratings/tidy-tuesday-chocolate-ratings.html",
    "title": "Tidy Tuesday: Chocolate Ratings",
    "section": "",
    "text": "Libraries\n\nlibrary(tidyverse)\nlibrary(nationalparkcolors)\nlibrary(paletteer)\nlibrary(tidytuesdayR)\nlibrary(extrafontdb)\nlibrary(showtextdb)\nlibrary(extrafont)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(ggridges)\nshowtext_auto()\ntheme_set(theme_minimal())\nfont_add_google(\"BenchNine\", family = \"BenchNine\")\n\n\n\nData\n\nchocolate &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv') %&gt;% \n  mutate(first_memorable_characteristics = str_remove(most_memorable_characteristics, pattern = \",.*\"))\n\n\n\nExploratory Data Visualization\n\nchocolate %&gt;% \n  count(company_location, sort = TRUE) %&gt;% \n  reactable::reactable()\n\n\n\n\n# Commonly used ingredients in chocolate manufacturing companies across the world\nchocolate %&gt;% \n  filter(!is.na(ingredients)) %&gt;% \n  add_count(ingredients) %&gt;% \n  filter(n&gt;= 100) %&gt;% \n  count(ingredients, sort = TRUE) %&gt;% \n  mutate(ingredients = ingredients %&gt;% fct_reorder(n)) %&gt;% \n  ggplot(aes(n, ingredients))+\n  geom_col(fill = \"#769370\", alpha = 0.8)+\n  labs(x = \"\", y = \"Ingredients\", \n       subtitle = \"&lt;span style = 'color:#454B68'&gt;B stands for Beans, S stands for Sugar, C stands for Cocoa Butter, L stands for Lecithin, and V stands for Vanilla&lt;/span&gt;\",\n       title = \"Commonly Used Ingredients by the Chocolate Manufacturing Companies\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(size = 17, face = \"bold\"), \n        plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\n\ntop_8_most_reviewed_countries &lt;- chocolate %&gt;% \n  count(company_location, sort = T) %&gt;% \n  filter(n &gt;= 50) %&gt;% \n  pull(company_location)\n\nchocolate %&gt;% \n  mutate(cocoa_percent = parse_number(cocoa_percent)) %&gt;% \n  add_count(company_location) %&gt;%\n  mutate(company_location_and_reviews = glue(\"{company_location} ({n})\")) %&gt;% \n  group_by(review_date, company_location_and_reviews, company_location) %&gt;% \n  summarize(categories = n(),\n            avg_rating     = mean(rating),\n            max_rated_manufacturer = first(company_manufacturer[which.max(rating)]),\n            min_rated_manufacturer = first(company_manufacturer[which.min(rating)]),\n            max_rated_manufacturer_rating = first(rating[which.max(rating)]),\n            min_rated_manufacturer_rating = first(rating[which.min(rating)])) %&gt;% \n  ungroup() %&gt;% \n  add_count(company_location) %&gt;% \n  mutate(company_location_and_reviews = company_location_and_reviews %&gt;% fct_reorder(-n)) %&gt;% \n  filter(company_location %in% top_8_most_reviewed_countries) %&gt;% \n  pivot_longer(cols = c(avg_rating, max_rated_manufacturer_rating, min_rated_manufacturer_rating), names_to = \"metrics\") %&gt;% \n  ggplot(aes(review_date, value, color = metrics))+\n  geom_line(size = 0.8)+\n  facet_wrap(~company_location_and_reviews)+\n  theme(text = element_text(family = \"BenchNine\"),\n        plot.title = element_text(size = 15))+\n  labs(x = \"\", \n       y = \"Ratings\",\n       title = \"Top 8 Frequently Reviewed Chocolate Manufacturing Countries w/ Ratings\",\n       subtitle = \"Number inside the parenthesis denotes the number of reviews conducted in said country, 2006-2021\",\n       color = \"Metrics\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")+\n  scale_color_paletteer_d(\"nationalparkcolors::SmokyMountains\")+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nWhere does most reviews come from?\n\nchocolate %&gt;% \n  add_count(company_location) %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(n &gt;= 50) %&gt;% \n  count(company_location, sort = T) %&gt;% \n  mutate(company_location = company_location %&gt;% fct_reorder(n)) %&gt;% \n  ggplot(aes(n, company_location))+\n  geom_col(fill = \"#769370\")+\n  labs(x = \"\", \n       y = \"\")+\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 200))+\n  labs(title = \"Top 8 Frequently Reviewed Chocolate Manufacturing Countries\",\n       subtitle = \"The review was conducted by The Manhattan Chocolate Society\",\n       caption   = \"Data Source: The Manhattan Chocolate Society\")+\n  theme(text = element_text(family = \"BenchNine\"),\n        legend.position = \"none\",\n        plot.title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nCommonly Used Ingredients in Chocolate Manufacturing Companies Across the World\n\nchocolate %&gt;%\n  mutate(company_location = company_location %&gt;% fct_lump(10)) %&gt;%\n  filter(!is.na(ingredients)) %&gt;%\n  add_count(ingredients) %&gt;%\n  filter(n &gt; 200) %&gt;%  # keeping ingredients that occur more than 100 times in the dataset\n  select(-n) %&gt;%\n  group_by(company_location) %&gt;%\n  add_count(ingredients) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    company_location = company_location %&gt;% fct_relevel(\n      c(\n        \"U.S.A.\",\n        \"Other\",\n        \"Canada\",\n        \"France\",\n        \"U.K.\",\n        \"Italy\",\n        \"Belgium\",\n        \"Ecuador\",\n        \"Australia\",\n        \"Switzerland\",\n        \"Germany\"\n      )\n    ),\n    company_location = company_location %&gt;% fct_reorder(n, .fun = sum)\n  ) %&gt;%\n  ggplot(aes(n, company_location, fill = ingredients)) +\n  geom_col(position = \"dodge\", width = 0.5) +\n  theme(\n    text = element_text(family = \"BenchNine\"),\n    legend.position = \"top\",\n    plot.title = element_text(size = 15, face = \"bold\")\n  ) +\n  scale_fill_paletteer_d(\"nationalparkcolors::SmokyMountains\") +\n  labs(y = \"\",\n       x = \"Frequency\",\n       fill = \"Ingredients\",\n       title = \"Frequently Used Ingredients in Chocolate Manufacturing Companies around the World\",\n       subtitle = \"B stands for Beans, S stands for Sugar, C stands for Cocoa Butter, and L stands for Lecithin\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")\n\n\n\n\n\n\n\n\n\n\nTen most frequently reviewed companies in the United States\n\nchocolate %&gt;% \n  filter(company_location == \"U.S.A.\") %&gt;% \n  add_count(company_manufacturer) %&gt;% \n  filter(n &gt;= 15) %&gt;% \n  count(company_manufacturer, sort = T) %&gt;% \n  mutate(company_manufacturer = company_manufacturer %&gt;% fct_reorder(n)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(n, company_manufacturer))+\n  geom_col(fill = \"#769370\")+\n  labs(x = \"\", \n       y = \"\", \n       title = \"Top 10 Frequently Reviewed Chocolate Manufacturing Companies in the United States\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")+\n  theme(text = element_text(family = \"BenchNine\"),\n        plot.title = element_text(size = 12.5))\n\n\n\n\n\n\n\n\n\n\nCocoa Percent Distribution\n\nIn the United States\n\nchocolate %&gt;% \n  filter(company_location == \"U.S.A.\") %&gt;% \n  mutate(cocoa_percent = parse_number(cocoa_percent)) %&gt;% \n  ggplot(aes(cocoa_percent))+\n  geom_histogram(color = \"white\", fill = \"#769370\")+\n  expand_limits(x = 50)+\n  labs(y = \"# of Chocolate Manufacturing Companies\", \n       x = \"Cocoa Percent\",\n       title = \"Cocoa Percent Distribution Histogram | U.S.A.\",\n       caption = \"Data Source: The Manhattan Chocolate Society\")+\n  theme(text = element_text(family = \"BenchNine\"),\n        plot.title = element_text(size = 20))+\n  scale_x_continuous(labels = scales::percent_format(scale = 1/1))\n\n\n\n\n\n\n\n\nFrom the above plot, it does look like most of the companies reviewed by The Manhattan Chocolate Society had cocoa in their products between 70% to 75%.\n\n\nAcross the World\n\nchocolate %&gt;% \n  mutate(cocoa_percent = parse_number(cocoa_percent)) %&gt;% \n  ggplot(aes(cocoa_percent))+\n  geom_histogram(color = \"white\", fill = \"#769370\")+\n  expand_limits(x = 40)+\n  labs(y = \"# of Chocolate Manufacturing Companies\", \n       x = \"Cocoa Percent\",\n       title = \"Cocoa Percent Distribution Histogram | Globally\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")+\n  theme(text = element_text(family = \"BenchNine\"),\n        plot.title = element_text(size = 20))+\n  scale_x_continuous(labels = scales::percent_format(scale = 1/1))\n\n\n\n\n\n\n\n\n\n\n\nWord Used to Describe the Chocolate in the USA\n\nchocolate %&gt;%\n  filter(company_location == \"U.S.A.\") %&gt;%\n  add_count(first_memorable_characteristics) %&gt;%\n  filter(n &gt; 28) %&gt;%\n  mutate(first_memorable_characteristics = glue(\"{first_memorable_characteristics} ({n})\"), \n         first_memorable_characteristics = first_memorable_characteristics %&gt;% fct_reorder(n)) %&gt;% \n  ggplot(\n    aes(rating, first_memorable_characteristics, fill = first_memorable_characteristics)\n  ) +\n  geom_density_ridges(alpha = 0.9) +\n  theme(legend.position = \"none\") +\n  scale_fill_paletteer_d(\"nationalparkcolors::GeneralGrant\")+\n  theme(text = element_text(family = \"BenchNine\"), \n        plot.title = element_text(size = 17, face = \"bold\"))+\n  labs(y = \"First Memorable Characteristic of the Chocolate\", \n       x = \"Rating\", \n       title = \"Distribution of Chocolate's First Memorable Characteristics in the USA\", \n       caption = \"Data Source: The Manhattan Chocolate Society\")\n\n\n\n\n\n\n\n\n\nfreq_reviewed_4_us_companies &lt;- chocolate %&gt;% \n  filter(company_location == \"U.S.A.\") %&gt;% \n  count(company_manufacturer, sort = T) %&gt;% \n  slice(1:4)\n\nfreq_reviewed_4_us_companies %&gt;% \n  reactable::reactable()\n\n\n\n\n\n\nchocolate %&gt;% \n  filter(company_location == \"U.S.A.\", \n         company_manufacturer %in% (freq_reviewed_4_us_companies %&gt;% pull(company_manufacturer))) %&gt;% \n  mutate(first_memorable_characteristics = first_memorable_characteristics %&gt;% fct_lump(4)) %&gt;% \n  count(company_manufacturer, first_memorable_characteristics, sort = T) %&gt;% \n  ggplot(aes(first_memorable_characteristics, n))+\n  geom_col(fill = \"#769370\")+\n  facet_wrap(~company_manufacturer, scales = \"free_y\")+\n  theme(text = element_text(family = \"BenchNine\"),\n        plot.title = element_text(size = 12.5))+\n  labs(x = \"Memorable Characteristics\", y = \"\", \n       title = \"Words used to describe the highly reviewed chocolate companies in the United States\",\n       caption = \"Data Source: The Manhattan Chocolate Society\")\n\n\n\n\n\n\n\n\nThe plot isn‚Äôt super informative, but Guittard products were described as ‚Äúcreamy‚Äù and ‚Äúsweet‚Äù more often than not."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html",
    "href": "posts/vllm-and-lmcache.html",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "",
    "text": "Every time I deploy a new LLM or optimize an existing one, I forget the details. How much memory does the KV cache actually use? What‚Äôs the formula again? How do I profile this? These are my notes so I can stop googling the same things over and over."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#why-im-writing-this",
    "href": "posts/vllm-and-lmcache.html#why-im-writing-this",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "",
    "text": "Every time I deploy a new LLM or optimize an existing one, I forget the details. How much memory does the KV cache actually use? What‚Äôs the formula again? How do I profile this? These are my notes so I can stop googling the same things over and over."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#vllm-what-it-does",
    "href": "posts/vllm-and-lmcache.html#vllm-what-it-does",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "vLLM: What It Does",
    "text": "vLLM: What It Does\nvLLM solves the memory management problem when serving LLMs. The big innovation is PagedAttention - think of it like this: instead of requiring continuous memory blocks (like needing an entire empty shelf for a book), it lets me split things across wherever there‚Äôs space. This sounds simple but it‚Äôs huge - I typically see 2-3x better throughput just from this.\nThe other major feature is continuous batching. Instead of waiting for an entire batch to finish, it dynamically fills slots as requests complete. Like a restaurant that seats new customers as tables open, not after everyone leaves.\nBottom line: same hardware, way more requests per second."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#the-kv-cache-math-i-keep-forgetting",
    "href": "posts/vllm-and-lmcache.html#the-kv-cache-math-i-keep-forgetting",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "The KV Cache Math I Keep Forgetting",
    "text": "The KV Cache Math I Keep Forgetting\nHere‚Äôs the part I always need to look up. For Llama 3.1 8B:\nPer token, I need: - 32 layers - 32 attention heads (grouped query attention with 8 KV heads) - 128 dimensions per head - √ó 2 (key and value) - √ó 2 bytes (FP16)\nFor the KV cache calculation with GQA: 2 √ó 32 √ó 8 √ó 128 √ó 2 = 131,072 bytes = 128 KB per token\nWait, that‚Äôs way less than I expected. This is because Llama 3.1 uses Grouped Query Attention (GQA) with only 8 KV heads instead of 32. This is one of the improvements over Llama 2.\nFor a 4096 token context: 128 KB √ó 4096 = ~512 MB\nSo for Llama 3.1 8B on my A100 80GB: - Model weights: ~16 GB (FP16) - KV cache for 4096 tokens: ~512 MB per request - Remaining: ~64 GB for batching more requests\nQuick rule: plan for 1-1.5 GB per concurrent user (accounts for varying generation lengths and overhead).\nHow many users can I serve? - 80 GB total - 16 GB for model - 64 GB remaining - At 1.2 GB per user: ~53 concurrent users theoretically - In practice, I target 35-40 concurrent requests\nThe GQA in Llama 3.1 makes a huge difference - way more efficient than Llama 2."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#lmcache-why-i-actually-use-it",
    "href": "posts/vllm-and-lmcache.html#lmcache-why-i-actually-use-it",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "LMCache: Why I Actually Use It",
    "text": "LMCache: Why I Actually Use It\nLMCache lets me cache KV pairs across requests. This matters because most of my applications have repeated prefixes:\nMy main use case: Extracting structured output from clinical notes - System prompt with extraction instructions: ~500 tokens - Few-shot examples: ~400 tokens - Total repeated prefix: ~900 tokens - Without LMCache: recompute 900 tokens for every clinical note - With LMCache: compute once, reuse for all notes\nFor 900 tokens on Llama 3.1 8B: 128 KB √ó 900 = ~115 MB saved per request\nThe clinical notes themselves vary wildly in length. Here‚Äôs the KV cache breakdown I need to plan for:\nKV Cache by Context Length (Llama 3.1 8B): - 4,096 tokens: 128 KB √ó 4096 = ~512 MB - 16,384 tokens (~16K): 128 KB √ó 16384 = ~2 GB - 32,768 tokens (~32K): 128 KB √ó 32768 = ~4 GB - 65,536 tokens (~64K): 128 KB √ó 65536 = ~8 GB\nThis matters for capacity planning. If I‚Äôm processing clinical notes that average 8K tokens but some go to 32K: - Average case: 512 MB + overhead = ~1 GB per user - Worst case: 4 GB + overhead = ~5 GB per user\nI need to plan for the worst case, so on my A100 80GB: - Model: 16 GB - Available for KV cache: 64 GB - Worst case (32K context): ~12-13 concurrent requests max - Average case (8K context): ~40 concurrent requests\nOther places I use LMCache: - RAG pipelines (same document context repeatedly) - Chat apps (long system prompts) - Any workflow with fixed instruction prefixes\nThe latency improvement is noticeable - first request is normal, subsequent requests are significantly faster because I skip computing those 900 tokens of instructions and examples."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#how-i-actually-profile-this",
    "href": "posts/vllm-and-lmcache.html#how-i-actually-profile-this",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "How I Actually Profile This",
    "text": "How I Actually Profile This\n\nQuick Check: nvidia-smi\nwatch -n 1 nvidia-smi\nSimple, gives me a rough idea. If I want more detail:\nnvidia-smi --query-gpu=memory.used,memory.free --format=csv -l 1\n\n\nMy Standard Profiling Script\nI keep this snippet handy:\nimport torch\nfrom vllm import LLM, SamplingParams\n\n# Reset memory tracking\ntorch.cuda.reset_peak_memory_stats()\n\n# Load model\nllm = LLM(\"meta-llama/Llama-3.1-8B-Instruct\",\n          gpu_memory_utilization=0.9,\n          max_model_len=8192)  # Llama 3.1 supports up to 128K but using 8K here\n\n# Test prompts\nprompts = [\n    \"Explain how neural networks learn.\",\n    \"What is the difference between supervised and unsupervised learning?\",\n    \"How does backpropagation work?\",\n    \"What are transformers in deep learning?\"\n]\n\nsampling_params = SamplingParams(temperature=0.8, max_tokens=256)\noutputs = llm.generate(prompts, sampling_params)\n\n# Memory breakdown\ntotal = torch.cuda.get_device_properties(0).total_memory / 1e9\npeak = torch.cuda.max_memory_allocated() / 1e9\nmodel_weights = 16  # Llama 3.1 8B in FP16\n\nprint(f\"\\nA100 80GB Memory Usage:\")\nprint(f\"Total: {total:.1f} GB\")\nprint(f\"Peak Used: {peak:.1f} GB\")\nprint(f\"Model: ~{model_weights:.1f} GB\")\nprint(f\"KV Cache: ~{peak - model_weights:.1f} GB\")\nprint(f\"Available: {total - peak:.1f} GB\\n\")\nTypical output I see: - Total: 80 GB - Model: 16 GB - KV cache for 4 requests: 2-3 GB (much less than Llama 2 thanks to GQA) - Remaining: ~60 GB\n\n\nUsing vLLM‚Äôs Built-in Stats\n# After running requests\nstats = llm.get_stats()\nprint(f\"KV Cache: {stats.get('gpu_cache_usage_perc', 0):.1f}%\")\nprint(f\"Cached Tokens: {stats.get('num_cached_tokens', 0)}\")\nThis is helpful to see how much of the allocated KV cache is actually being used."
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#my-deployment-notes-for-a100-80gb",
    "href": "posts/vllm-and-lmcache.html#my-deployment-notes-for-a100-80gb",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "My Deployment Notes for A100 80GB",
    "text": "My Deployment Notes for A100 80GB\nWhat works well: - Llama 3.1 8B: Easy, can batch 35-40 users comfortably - Llama 3.1 70B: Need multi-GPU with tensor parallelism (2-3 A100s)\nMemory formula I use:\nTotal needed = Model weights + (1.2 GB √ó concurrent users) + 5 GB buffer\nFor Llama 3.1 8B:\n16 GB + (1.2 √ó 40) + 5 = ~69 GB\nSo ~40 concurrent users is my target for 8K context. For shorter contexts (2K), I can push to 50+.\nFor Llama 3.1 70B (single GPU won‚Äôt fit, just for reference):\n~140 GB in FP16 - needs 2x A100 80GB minimum"
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#performance-numbers-ive-seen",
    "href": "posts/vllm-and-lmcache.html#performance-numbers-ive-seen",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "Performance Numbers I‚Äôve Seen",
    "text": "Performance Numbers I‚Äôve Seen\nBefore vLLM (using HuggingFace pipeline): - ~8 requests/second - Could batch 4-5 requests max - Lots of wasted memory\nAfter vLLM with Llama 3.1 8B: - ~40 requests/second - Batch 35-40 requests - Much better memory utilization (GQA helps a lot)\nAfter adding LMCache (for RAG app): - First request: same latency - Subsequent requests: ~45% faster - Way better throughput because I‚Äôm skipping 500 tokens of repeated context"
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#things-i-learned-the-hard-way",
    "href": "posts/vllm-and-lmcache.html#things-i-learned-the-hard-way",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "Things I Learned the Hard Way",
    "text": "Things I Learned the Hard Way\n\nContext length matters more than batch size for memory\n\nOne user with 8192 tokens uses way more memory than four users with 2048 tokens each\nAlways ask: what‚Äôs my average context length?\nLlama 3.1 supports up to 128K context but I rarely use more than 8K\n\nLeave headroom\n\nDon‚Äôt use 100% of memory\nSpikes happen during generation\nI target 90-95% max\n\nLMCache requires shared prefixes\n\nIf every request is unique, LMCache doesn‚Äôt help\nCheck my use case first\n\nMonitor in production\n\nMemory usage varies with load\nWhat works in testing might not scale\nI keep nvidia-smi running in a tmux window\n\nGQA is a game changer\n\nLlama 3.1‚Äôs grouped query attention uses way less KV cache\nThis means more concurrent users or longer contexts"
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#quick-reference-model-sizes-fp16",
    "href": "posts/vllm-and-lmcache.html#quick-reference-model-sizes-fp16",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "Quick Reference: Model Sizes (FP16)",
    "text": "Quick Reference: Model Sizes (FP16)\nFor when I need to quickly estimate: - Llama 3.1 8B: ~16 GB - Llama 3.1 70B: ~140 GB (multi-GPU) - Mistral 7B v0.3: ~14 GB - Mixtral 8x7B: ~90 GB - Qwen 2.5 7B: ~14 GB"
  },
  {
    "objectID": "posts/vllm-and-lmcache.html#when-i-come-back-to-this",
    "href": "posts/vllm-and-lmcache.html#when-i-come-back-to-this",
    "title": "vLLM and LMCache: Notes on Optimizing LLM Inference",
    "section": "When I Come Back to This",
    "text": "When I Come Back to This\nKey things to remember: - Llama 3.1 8B uses GQA: only 128 KB per token (vs 512 KB for Llama 2) - Plan 1-1.5 GB per concurrent requests for Llama 3.1 8B - Use LMCache for repeated prefixes - Always leave 5-10 GB buffer\nMore to add‚Ä¶"
  },
  {
    "objectID": "posts/2022-03-28-dairyplantdata-r-data-package/dairyplantdata-r-data-package.html",
    "href": "posts/2022-03-28-dairyplantdata-r-data-package/dairyplantdata-r-data-package.html",
    "title": "{dairyplantdata} R data package",
    "section": "",
    "text": "About the Project\nThe purpose behind building {dairyplantdata} data package was to facilitate senior economists at the University of Nebraska - Lincoln to investigate the Economic Impact of Dairy Processing Expansion in Nebraska. It was also built to remove the hassle of searching datasets in some corner of our computer.\nI personally did not want to go through multiple folders searching for a dataset, so I was more motivated to build this into a data package.\n\nLearn More: Economic Impact of Dairy Processing Expansion in Nebraska\nYou can access the usage information here: dairyplantdata\n\n\n\n\nHow to use?\n\nRun the following piece of code:\n\n\n# install.package(\"remotes\")   #In case you have not installed it.\nremotes::install_github(\"nBhabish/dairyplantdata\")\n\ndairyplantdata::dairy_plant_data"
  },
  {
    "objectID": "posts/2022-03-28-shiny-application-united-states-natural-gas-demand-forecast/shiny-application-united-states-natural-gas-demand-forecast.html",
    "href": "posts/2022-03-28-shiny-application-united-states-natural-gas-demand-forecast/shiny-application-united-states-natural-gas-demand-forecast.html",
    "title": "R Shiny Application: Natural Gas Demand Forecast",
    "section": "",
    "text": "Dashboard Link\nLink to the Shiny Application: Natural Gas Demand Forecast\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaways\n\nOut of all of the models, auto_arima() gave us the best forecasting accuracy.\nTBATS was the worst performing model.\nPost-forecast diagnostics inform that most of the information was extracted by the fitted models.\n\n\n\nImprovisation\n\nML models like XGBoost, or a combination of XGBoost and PROPHET could potentially give us a better accuracy. XGBoost by itself is great at picking up patterns, but when combined with PROPHET (given the seasonality parameter is turned off for PROPHET), it would be ideal at picking up multiple seasonalities.\nAbove recommendation is a potential method and by no means a solution. The best way to find out would be to fit the model and observe the accuracy metrics."
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "You can subscribe to updates via RSS or email.\n\nRSS Feed\nEmail updates coming soon."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Data Engineer at Medical College of Wisconsin, where I build data pipelines and infrastructure to support healthcare analytics and research.\n\nInvolvements: AN.ai Lab, Seo Lab"
  },
  {
    "objectID": "about.html#currently",
    "href": "about.html#currently",
    "title": "About Me",
    "section": "",
    "text": "Data Engineer at Medical College of Wisconsin, where I build data pipelines and infrastructure to support healthcare analytics and research.\n\nInvolvements: AN.ai Lab, Seo Lab"
  },
  {
    "objectID": "about.html#past-experiences",
    "href": "about.html#past-experiences",
    "title": "About Me",
    "section": "Past Experiences",
    "text": "Past Experiences\nEcolab, Product Data Analyst\n- Developed data-driven solutions for operational efficiency and business intelligence.\nBureau of Business Research, Research Analyst\n- Conducted economic research and statistical analysis."
  },
  {
    "objectID": "about.html#research-publications",
    "href": "about.html#research-publications",
    "title": "About Me",
    "section": "Research Publications",
    "text": "Research Publications\n\nJournal of Clinical Oncology (2025)\n\nAssociation Between Area Deprivation Index and Survival in Patients With Advanced Biliary Tract Cancers\n\nbioRxiv (2025)\n\nCHIMERA-DDR: A Machine Learning Framework for Classifying Heterogeneous Mismatch-Repair and Homologous-Recombination Deficiency Patterns in Prostate Cancer\nMoving Beyond Binary Biomarkers: Machine Learning Model Resolves Concurrent and Molecularly Heterogeneous Mismatch-Repair and Homologous-Recombination Deficiencies in Prostate Cancer\n\nJAMA Surgery (2024)\n\nApplying Large Language Models for Surgical Case Length Prediction\n\nSurgery (2024)\n\nDevelopment and validation of an artificial intelligence system for surgical case length prediction"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nUniversity of Nebraska-Lincoln - B.A. Economics & Mathematics"
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About Me",
    "section": "Get in Touch",
    "text": "Get in Touch\nFeel free to connect with me:\n\nWebsite: bhabishya.rbind.io\nEmail: bhabishyaneupane45@gmail.com\nGitHub: github.com/nBhabish"
  }
]